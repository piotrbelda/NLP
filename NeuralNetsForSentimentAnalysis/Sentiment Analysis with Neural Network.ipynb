{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random as rnd\n",
    "import trax\n",
    "import trax.fastmath.numpy as np\n",
    "from trax import layers as tl\n",
    "from utils import Layer, load_tweets, process_tweet\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of positive tweets: 5000\n",
      "The number of negative tweets: 5000\n"
     ]
    }
   ],
   "source": [
    "all_positive_tweets,all_negative_tweets=load_tweets()\n",
    "print(f\"The number of positive tweets: {len(all_positive_tweets)}\")\n",
    "print(f\"The number of negative tweets: {len(all_negative_tweets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pos=all_positive_tweets[4000:]\n",
    "train_pos=all_positive_tweets[:4000]\n",
    "\n",
    "val_neg=all_negative_tweets[4000:]\n",
    "train_neg=all_negative_tweets[:4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=train_pos+train_neg\n",
    "val_x=val_pos+val_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y=np.append(np.ones(len(train_pos)),np.ones(len(train_neg)))\n",
    "val_y=np.append(np.ones(len(val_pos)),np.ones(len(val_neg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train_x: 8000\n",
      "length of val_x: 2000\n"
     ]
    }
   ],
   "source": [
    "print(f\"length of train_x: {len(train_x)}\")\n",
    "print(f\"length of val_x: {len(val_x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet before and after preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original tweet at training position 0:\n",
      "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "\n",
      "\n",
      "['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "print(\"original tweet at training position 0:\")\n",
    "print(train_pos[0])\n",
    "print('\\n')\n",
    "print(process_tweet(train_pos[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building vocabulary mapping each word to an integer index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocab={\"__PAD__\":0,\"__</e>__\":1,\"__UNK__\":2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in train_x:\n",
    "    processed_tweet=process_tweet(tweet)\n",
    "    for word in processed_tweet:\n",
    "        if word not in Vocab:\n",
    "            Vocab[word]=len(Vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in vocab are:  9088\n"
     ]
    }
   ],
   "source": [
    "print(\"Total words in vocab are: \",len(Vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting tweet to a tensor, consisting of unique indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_to_tensor(tweet,vocab_dict,unk_token='__UNK__',verbose=False):\n",
    "    word_l=process_tweet(tweet)\n",
    "    if verbose:\n",
    "        print('List of words from the processed tweet: ')\n",
    "        print(word_l)\n",
    "    tensor_l=[]\n",
    "    unk_ID=vocab_dict[unk_token]\n",
    "    for word in word_l:\n",
    "        word_ID=vocab_dict.get(word,unk_ID)\n",
    "        tensor_l.append(word_ID)\n",
    "    return tensor_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual tweet is:\n",
      " Bro:U wan cut hair anot,ur hair long Liao bo\n",
      "Me:since ord liao,take it easy lor treat as save $ leave it longer :)\n",
      "Bro:LOL Sibei xialan\n",
      "\n",
      "Tensor of tweet:\n",
      " [1065, 136, 479, 2351, 745, 8148, 1123, 745, 53, 2, 2672, 791, 2, 2, 349, 601, 2, 3489, 1017, 597, 4559, 9, 1065, 157, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "print('Actual tweet is:\\n',val_pos[0])\n",
    "print('\\nTensor of tweet:\\n',tweet_to_tensor(val_pos[0],vocab_dict=Vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(data_pos, data_neg, batch_size, loop, vocab_dict, shuffle=False):\n",
    "\n",
    "    n_to_take = batch_size // 2\n",
    "    \n",
    "    pos_index = 0\n",
    "    neg_index = 0\n",
    "    \n",
    "    len_data_pos = len(data_pos)\n",
    "    len_data_neg = len(data_neg)\n",
    "    \n",
    "    pos_index_lines = list(range(len_data_pos))\n",
    "    neg_index_lines = list(range(len_data_neg))\n",
    "    \n",
    "    if shuffle:\n",
    "        rnd.shuffle(pos_index_lines)\n",
    "        rnd.shuffle(neg_index_lines)\n",
    "        \n",
    "    stop = False\n",
    "    \n",
    "    while not stop:  \n",
    "        \n",
    "        batch = []\n",
    "        \n",
    "        for i in range(n_to_take):\n",
    "                    \n",
    "            if pos_index >= len_data_pos: \n",
    "                \n",
    "                if not loop:\n",
    "                    stop = True;\n",
    "                    break;\n",
    "                \n",
    "                pos_index = 0\n",
    "                \n",
    "                if shuffle:\n",
    "                    rnd.shuffle(pos_index_lines)\n",
    "                    \n",
    "            tweet = data_pos[pos_index_lines[pos_index]]\n",
    "            \n",
    "            tensor = tweet_to_tensor(tweet, vocab_dict)\n",
    "            \n",
    "            batch.append(tensor)\n",
    "            \n",
    "            pos_index = pos_index + 1\n",
    "\n",
    "        for i in range(n_to_take):\n",
    "            \n",
    "            if neg_index>=len_data_neg:\n",
    "                \n",
    "                if not loop:\n",
    "                    stop = True;\n",
    "                    break;\n",
    "                    \n",
    "                neg_index = 0\n",
    "                \n",
    "                if shuffle:\n",
    "                    rnd.shuffle(neg_index_lines)\n",
    "            tweet = data_neg[neg_index_lines[neg_index]]\n",
    "            \n",
    "            tensor = tweet_to_tensor(tweet,vocab_dict)\n",
    "            \n",
    "            batch.append(tensor)\n",
    "            \n",
    "            neg_index = neg_index + 1\n",
    "\n",
    "        if stop:\n",
    "            break;\n",
    "\n",
    "        pos_index += n_to_take\n",
    "        \n",
    "        neg_index += n_to_take\n",
    "        \n",
    "        max_len = max([len(t) for t in batch]) \n",
    "        \n",
    "        tensor_pad_l = []\n",
    "        \n",
    "        for tensor in batch:\n",
    "\n",
    "            n_pad = max_len - len(tensor)\n",
    "            \n",
    "            pad_l = [0] * n_pad\n",
    "            \n",
    "            tensor_pad = tensor + pad_l\n",
    "            \n",
    "            tensor_pad_l.append(tensor_pad)\n",
    "\n",
    "        inputs = np.asarray(tensor_pad_l)\n",
    "  \n",
    "        target_pos = [1] * n_to_take\n",
    "        \n",
    "        target_neg = [0] * n_to_take\n",
    "        \n",
    "        target_l = target_pos + target_neg\n",
    "        \n",
    "        targets = np.asarray(target_l)\n",
    "\n",
    "        example_weights = np.ones_like(targets)\n",
    "        \n",
    "        yield inputs, targets, example_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing batch generator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: [[2005 4451 3201    9    0    0    0    0    0    0    0]\n",
      " [4954  567 2000 1454 5174 3499  141 3499  130  459    9]\n",
      " [3761  109  136  583 2930 3969    0    0    0    0    0]\n",
      " [ 250 3761    0    0    0    0    0    0    0    0    0]]\n",
      "Targets: [1 1 0 0]\n",
      "Example Weights: [1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "rnd.seed(30) \n",
    "\n",
    "def train_generator(batch_size, shuffle = False):\n",
    "    return data_generator(train_pos, train_neg, batch_size, True, Vocab, shuffle)\n",
    "\n",
    "def val_generator(batch_size, shuffle = False):\n",
    "    return data_generator(val_pos, val_neg, batch_size, True, Vocab, shuffle)\n",
    "\n",
    "def test_generator(batch_size, shuffle = False):\n",
    "    return data_generator(val_pos, val_neg, batch_size, False, Vocab, shuffle)\n",
    "\n",
    "inputs, targets, example_weights = next(train_generator(4, shuffle=True))\n",
    "\n",
    "print(f'Inputs: {inputs}')\n",
    "print(f'Targets: {targets}')\n",
    "print(f'Example Weights: {example_weights}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining own Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu(Layer):\n",
    "    def forward(self,X):\n",
    "        activation=np.maximum(X,0)\n",
    "        return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array([[0,-1,1],[2,-2,0]],dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu_layer=Relu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input array: [[ 0. -1.  1.]\n",
      " [ 2. -2.  0.]]\n",
      "ReLU activation values: [[0. 0. 1.]\n",
      " [2. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(f'Input array: {x}')\n",
    "print(f'ReLU activation values: {relu_layer(x)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using fastmath modules within Trax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trax import fastmath\n",
    "np=fastmath.numpy\n",
    "random=fastmath.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self,n_units,init_stdev=.1):\n",
    "        self._n_units=n_units\n",
    "        self._init_stdev=init_stdev\n",
    "    def forward(self,X):\n",
    "        dense=np.dot(X,self)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
